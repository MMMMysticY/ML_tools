{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 关于机器学习中的mask\n",
    "在类似于transformer等模型中，不论是进行NLP任务还是CV任务，都会有可能碰到有关mask的问题。\n",
    "mask往往见于MLM、通过transformer encoder进行NLP任务中的pad mask、transformer decoder的seq mask、图像上的mae等等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## multihead-self-attention中的pad mask\n",
    "在multihead attention中，attention_score往往是[batch, head, seq_len, seq_len]维度，表示的是对于batch中每个样本，每个head上，每个位置对于包括自己的共seq_len个位置的注意力。\n",
    "而pad mask往往是[batch, seq_len]维度，表示的是对于每个样本，在长为seq_len的位置上，哪几个是pad token，需要mask而不参与attention计算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 下方为multihead self-attention的经典基本实现（仅有pad mask）\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    # multi-head attention\n",
    "    def __init__(self, head, dim, dropout=0.):\n",
    "        \"\"\"\n",
    "        head: 头个数\n",
    "        dim: 原始维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dim % head == 0\n",
    "        self.head = head\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // head\n",
    "        self.to_qkv = nn.Linear(self.dim, 3 * self.dim, bias=False)\n",
    "        # 将qkv用一个linear一起完成\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.attend = nn.Softmax(-1)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: [B, S, E]\n",
    "        # mask: [B, S]\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        # 经过to_qkv计算得到[B, S, 3*E]， 之后chunk分割得到qkv: [3, B S, E]\n",
    "\n",
    "        query, key, value = map(lambda t: rearrange(t, 'b s (h d) -> b h s d', h=self.head), qkv)\n",
    "        # 对于每个[b, s, e]的对象 分割为[b, s, h, d]再转置为[b h s d]\n",
    "\n",
    "        attention_score = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        # [B, head, S, S]\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_score = attention_score.masked_fill(mask == 0, -1e9)\n",
    "            # mask矩阵中为0为需要mask的地方 mask值为负无穷则softmax为0\n",
    "        attention = self.attend(attention_score)\n",
    "\n",
    "        output = torch.matmul(attention, value)\n",
    "        # [B, head, S, head_dim]\n",
    "        output = rearrange(output, 'b h s d -> b s (h d)')\n",
    "\n",
    "        output = self.output_proj(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2, 8])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_object = Attention(head=2, dim=8)\n",
    "random_input = torch.randn(size=(3, 2, 8))\n",
    "mask = None\n",
    "output = attention_object(random_input, mask)\n",
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在mask为None的时候，正常计算不报错"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-3483ad93a868>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mrandom_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m8\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmask\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mattention_object\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrandom_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\python3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-10-0a12d678f899>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x, mask)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmask\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 39\u001B[1;33m             \u001B[0mattention_score\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mattention_score\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmasked_fill\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1e9\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     40\u001B[0m             \u001B[1;31m# mask矩阵中为0为需要mask的地方 mask值为负无穷则softmax为0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[0mattention\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mattend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mattention_score\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "attention_object = Attention(head=2, dim=8)\n",
    "random_input = torch.randn(size=(3, 2, 8))\n",
    "mask = torch.randint(0, 2, size=(3, 2)) == 0.\n",
    "output = attention_object(random_input, mask)\n",
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "值得注意的是，在加入mask后就报错了。 这是一个广播上的报错 masked_fill方法是通过广播机制实现\n",
    "此时attention_score为[3,2,2,2] 而mask为[3,2] 不满足广播条件。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "此时，需要让mask满足广播条件，应该将其扩展为[3,1,1,2]。\n",
    "该过程的理解为：\n",
    "1. 3与3相同是batch维度，每个样本代表自己的没必要进行广播；\n",
    "2. 原始mask的第二维为2 是seq_len长度，就是对于一个样本而言，sequence上哪些位置是需要mask的，attention的最后两维均为seq_len，表示的是对于每个位置，相对于包括自己的所有位置的attention注意力值\n",
    "3. 那么需要将其在导数第二维进行扩充，变为[3,1,2]表示的是对于每个位置，都需要mask掉一些位置\n",
    "4. 之后，对于每个head而言，都是完全相同的计算，现在有两个head，每个head都是seq_len*seq_len的注意力，那么就将mask扩展到[3,1,1,2]上，满足广播条件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2, 8])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_object = Attention(head=2, dim=8)\n",
    "random_input = torch.randn(size=(3, 2, 8))\n",
    "\n",
    "mask = torch.randint(0, 2, size=(3, 2)) == 0.\n",
    "mask = rearrange(mask, 'b s -> b 1 1 s')\n",
    "# 扩充维度 满足广播条件\n",
    "\n",
    "output = attention_object(random_input, mask)\n",
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}