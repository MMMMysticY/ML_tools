# mask
mask往往用于在矩阵运算过程中掩盖掉一些不能参与计算的地方。
pytorch中往往使用masked_fill方法进行实现，见[masked_fill api](https://pytorch.org/docs/1.11/generated/torch.Tensor.masked_fill_.html?highlight=masked_fill#torch.Tensor.masked_fill_)  
第一个参数为mask矩阵，要求BoolTensor，**其中True为需要mask的地方；另外，mask矩阵需要和被mask的矩阵满足广播条件**。  
第二个参数为value，即mask位置需要赋予的值，如果后续使用softmax函数，则往往是一个巨小值如-1e9等。  

## self-attention中的pad mask
pad mask往往是为了将具有seq_len的一个样本，扩展到max_seq_len上，能够作为一个batch进行计算。  
此时需要为其在不足的序列长度上添加pad token。  
而在self-attention计算中，就存在mask的使用。  
### pad mask的理解
加入一个self-attention操作，max_seq_len=5，而一个文本只有3，则需要将其mask至5进行计算。  
![pad mask1](img/pad_mask1.png)  
如上图，计算了query和key 得到了attention_score，  
而此时不能对其直接softmax，因为存在不要考虑的pad部分，需要将其mask掉。  
每一行对应一个seq位置，而五列则是对于五个位置的考虑程度，此时需要mask最右侧两列  
![pad mask2](img/pad_mask2.png)  
这样在做softmax的时候，就不会考虑后面两个pad token。  

此时有一个问题，红色框部分要不要变为-inf和0  
![pad mask3](img/pad_mask3.png)  
在下面这个推导中，可以得出的结论是，红框mask与不mask对前三个有意义的位置没有任何影响，而对于seq_len上最后的pad位置，则完全不一样。  
![pad mask4](img/pad_mask4.png)  
需要注意的是，上图推导中，最为重要的部分就是由于softmax将值变为0，而导致了有意义的部分两种操作的结果完全相同。  
### pad mask的实现与广播
见[mask代码](mask.ipynb)  