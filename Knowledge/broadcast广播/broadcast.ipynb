{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# numpy中的广播"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 4., 10., 18.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一般情况下 矩阵的加减和点积需要在相同维度的两个矩阵上进行\n",
    "\n",
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([4.0, 5.0, 6.0])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 4.,  8., 12.])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下方是一个简单的广播样例\n",
    "\n",
    "c = np.array([1.0, 2.0, 3.0])\n",
    "d = 4.0\n",
    "c*d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 1.,  2.,  3.],\n       [11., 12., 13.],\n       [21., 22., 23.],\n       [31., 32., 33.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (4,1)与(3)进行广播 相当于(4,1)与(1,3)广播 结果为(4,3)\n",
    "\n",
    "a = np.array([0.0, 10.0, 20.0, 30.0])\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "c = a[:, np.newaxis] + b\n",
    "print(c.shape)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pytorch中的广播"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "a = torch.tensor([0.0, 10.0, 20.0, 30.0])\n",
    "a = a.unsqueeze(dim=1)\n",
    "b = torch.tensor([1.0, 2.0, 3.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(b.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a*b\n",
    "c.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## masked_fill使用广播"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 假设一个tensor x进行self-attention计算 已经计算得到key query\n",
    "query = torch.randn(size=(2, 3, 8))\n",
    "key = torch.randn(size=(2, 3, 8))\n",
    "# query与key维度为[b, s, d] batch=2 seq_len=3 d_model=8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 3])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算attention score\n",
    "attention_score = torch.matmul(query, key.transpose(-1, -2))\n",
    "attention_score.shape\n",
    "# attention_score为2,3,3 即对于seq上的每个位置 都对其他三个位置有attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 而对于tensor有pad mask\n",
    "mask = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                     [1.0, 1.0, 0.0]])\n",
    "mask.shape\n",
    "# 其中0.0表示需要mask的位置 1.0表示不需要mask的位置\n",
    "# mask的含义是batch1的后面两个token都需要mask  batch2的第三个token需要mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False,  True,  True],\n        [False, False,  True]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_bool = mask == 0.\n",
    "mask_bool"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-7c9dfd3fa81d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# 现在attention score与mask_bool不满足广播要求\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mattention_score\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmasked_fill\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask_bool\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1e9\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 现在attention score与mask_bool不满足广播要求\n",
    "attention_score.masked_fill(mask_bool, -1e9)\n",
    "\n",
    "# 因为attention_score为(2,3,3) 而mask_bool为(2,3)\n",
    "# 从右向左看3与2不相等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 3])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 此时从mask本来含义入手，mask的第一个值需要对第一个batch中的每一个seq位置的第二个和第三位置遮盖\n",
    "# 对于seq=0的三个注意力值而言，需要对第二个和第三个mask 对于seq=1和2也完全相同\n",
    "# 那么将mask_bool在seq维度上扩展，让其广播至每个seq_len上即可\n",
    "mask_bool_ = mask_bool.unsqueeze(dim=1)\n",
    "mask_bool_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.7514e+00, -1.0000e+09, -1.0000e+09],\n         [ 1.4653e+00, -1.0000e+09, -1.0000e+09],\n         [ 4.5117e+00, -1.0000e+09, -1.0000e+09]],\n\n        [[-1.5321e+00, -1.3974e+00, -1.0000e+09],\n         [ 5.8771e+00, -2.3576e+00, -1.0000e+09],\n         [-5.3886e-01,  3.5407e+00, -1.0000e+09]]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score_ = attention_score.masked_fill(mask_bool_, -1e9)\n",
    "attention_score_\n",
    "# 满足条件"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}